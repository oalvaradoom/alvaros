{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = \"...\" # Insertar API de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cadenas \n",
    "Las cadenas en LangChain funcionan para poder crear flujos de trabajo, donde unimos distintos \"bloques\" para poder crear un sistemas con LLMs mas complejo.\n",
    "\n",
    "Por ejemplo, si quieres crear un sistema el cual conecte distintas entradas y salidas de LLMs entre si con las cadenas puedes gestionar que modelo genera que información, con que prompt y la salida de ese modelo si funciona como entrada a otro.\n",
    "\n",
    "Hay bastantes cadenas que [LangChain tiene por default](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chains) que puedes usar como bloques para crear tu sistema. Pero tambien puedes crear tus propias cadenas al igual que descargar distintas cadenas que ha hecho la comunidad desde el [LangChain-Hub](https://github.com/hwchase17/langchain-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadenas más usadas\n",
    "Hay bastantes cadenas las cuales estan integradas dentro de LangChain, pero en este cuadeno estaremos viendo las mas cómunes que te serán muy útiles para poder desarrollar distintos sistemas.\n",
    "\n",
    "* LLMChain\n",
    "* SequentialChain\n",
    "* Math/transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain\n",
    "LLMChain es una de las cadenas que más usaras. Lo que hace es unir dos elementos para que puedas interactuar con las LLMs de manera mas sencilla. \n",
    "\n",
    "Una un modelo LLM (Puede ser LLama, OpenAI, Cohere etc.) y los templates de prompts vistos en el cuaderno [intro.ipynb](intro.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "\n",
    "prompt = '''Eres un asistente virtual experto en {tema} y respondes \n",
    "            con una lista de 3 conceptos clave sobre el mismo\n",
    "            Solo enumeras los tres conceptos'''\n",
    "template = PromptTemplate.from_template(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=API)\n",
    "cadena_LLM = LLMChain(llm=llm, prompt=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cadena es el elemento con el que vas a intecartuar ahora para poder hacer predicciones.\n",
    "\n",
    "Lo que hace ahora esta cadena es tomar tu input (El tema), darle formato al prompt que usará y envia el prompt construido al modelo para su generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadena_LLM.predict(tema=\"ingenieria civil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "Para muchos casos de usos solo enviar un texto para ser procesado no es suficiente, por lo que se requiere de una secuencia de procesos que se ejecuten en orden. Para esto se puede utilizar las cadenas `SimpleSequentialChain o SequentialChain` que permiten encadenar varios procesos de manera secuencial.\n",
    "\n",
    "Cuando quieres que la salida que genera el modelo funcione como entrada para otro, este es una gran manera de hacerlo. \n",
    "\n",
    "En este ejemplo veremos SequentialChain, ya que nos brinda mas flexibilidad que SimpleSequentialChain, pues puede recibir multiples entradas y generar multiples salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos la primera cadena que será la misma que hicimos en el ejercicio anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=API)\n",
    "prompt = '''Eres un asistente virtual experto en {tema} y respondes \n",
    "            con una lista de 3 conceptos clave sobre el mismo\n",
    "            Solo enumeras los tres conceptos'''\n",
    "template = PromptTemplate.from_template(prompt)\n",
    "cadena_lista = LLMChain(llm=llm, prompt=template, output_key=\"lista_conceptos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos una cadena la cual va a recibir la salida de la cadena cadena_LLM y lo procesa para generar otro texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Eres un asistente virtual que recibe una lista de conceptos \n",
    "            de un area de conocimiento y \n",
    "            debe devolver cual de esos conceptos es mejor aprender primero.\n",
    "            Los conceptos son: {lista_conceptos}'''\n",
    "template = PromptTemplate.from_template(prompt)\n",
    "cadena_inicio = LLMChain(llm=llm, prompt=template, output_key=\"donde_iniciar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "cadenas = SequentialChain(chains=[cadena_lista, cadena_inicio],\n",
    "                          input_variables=[\"tema\"],\n",
    "                          output_variables=[\"lista_conceptos\", \"donde_iniciar\"], \n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadenas({\"tema\": \"fisica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "cadena_simple = SimpleSequentialChain(chains=[cadena_lista, cadena_inicio], verbose=True)\n",
    "cadena_simple.run(\"Inteligencia artificial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MathChain\n",
    "from langchain import LLMMathChain\n",
    "cadena_mate = LLMMathChain(llm=llm, verbose=True)\n",
    "cadena_mate.run(\"Cuanto es 432*12-32+32?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformChain\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "\n",
    "def eliminar_brincos(input):\n",
    "    \"\"\"Elimina los brincos de línea de un texto.\"\"\"\n",
    "    texto = input[\"texto\"]\n",
    "    return {\"texto_limpio\": texto.replace(\"\\n\", \" \")}\n",
    "\n",
    "\n",
    "cadena_transformacion = TransformChain(input_variables=[\"texto\"],\n",
    "                                        output_variables=[\"texto_limpio\"],\n",
    "                                        transform=eliminar_brincos)\n",
    "\n",
    "prompt = '''\\nEste es un texto \\ncon brincos de\\n línea.\\n\\n'''\n",
    "\n",
    "\n",
    "cadena_transformacion.run(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
