{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos\n",
    "Estos son los modelos que se encargarán de procesar nuestro texto. En el caso de usar OpenAI, se tiene que crear una variable de ambiente llamada \"OPENAI_API_KEY\" con el API KEY de OpenAI.\n",
    "\n",
    "Para hacer esto en linux se tiene que correr desde la terminal\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"insertar_api_key_aqui\"\n",
    "```\n",
    "\n",
    "O tambien se puede agregar a un archivo config.py que unicamente contiene la variable del API Key\n",
    "\n",
    "```python\n",
    "OPENAI_API_KEY=\"insertar_api_key_aqui\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI \n",
    "import config\n",
    "api = config.OPENAI_API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = OpenAI(model_name = \"text-davinci-003\", openai_api_key=api)\n",
    "#llm_llama = LlamaCpp(model_path=\"./llamacpp/models/7B/ggml-model-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(\"Hola, como estas?\")\n",
    "#respuesta_llama = llm_llama(\"Hola, como estas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta_openai)\n",
    "#print(respuesta_llama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Chat\n",
    "Los modelos que utilizamos eran para comletar texto, sin embargo podemos tambien usar modelos especificos para chat como ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(openai_api_key=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = chatgpt([HumanMessage(content=\"Hola, como estas?\")])\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "El prompt es el texto que se le envía al modelo similar a lo visto en la sección pasada. El éxito de los prompts está en que cuenten con una buena estructura y que den suficiente contexto el modelo para que entienda que esperamos de respuesta.\n",
    "Cada prompt que le enviemos al modelo debe de tener una estructura similar, Langchain tiene un par de herramientas que nos facilitas esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este template solo tenemos que poner el promt y entre llaves {} pones las variables\n",
    "# que quieres que se reemplacen\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template_basico = \"\"\"Eres un asistente virtual culinario que responde a preguntas\n",
    "de manera muy breve.\n",
    "Pregunta: Cuales son los ingredientes para preparar {platillo}\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constuimos el template, especificandole cuales son las variables de entrada y cual es el texto que tiene que usar\n",
    "prompt_temp = PromptTemplate(input_variables=[\"platillo\"], template = template_basico)\n",
    "\n",
    "#Aqui podemos ver como se reemplaza la variable platillo por tacos al pastor\n",
    "promt_value = prompt_temp.format(platillo=\"tacos al pastor\")\n",
    "print(promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_openai = llm_openai(promt_value)\n",
    "print(respuesta_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede revisar el numero de tokens de un prompt en especifico\n",
    "# Solo requieres instalar tiktoken con pip install tiktoken\n",
    "llm_openai.get_num_tokens(promt_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplates\n",
    "AL igual que tenemos templates para modelos abiertos, LangChain tambien nos brinda templates para modelos de chat. Estos templates nos ayudan a darle la informacion a los modelos de chat en la manera en la que lo necesitan.\n",
    "\n",
    "Los elementos de estos templates son:\n",
    "* **Human**: El texto que escribimos nosotros\n",
    "* **AI**: El texto que responde el modelo\n",
    "* **System**: El texto que se le envía al modelo para darle contexto de su funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armemos el template para el sistema\n",
    "prompt_temp_sistema = PromptTemplate(\n",
    "    template=\"Eres un asistente virtual que me recomienda una alternativa {adjetivo} a un producto\",\n",
    "    input_variables=[\"adjetivo\"],\n",
    ")\n",
    "\n",
    "template_sistema = SystemMessagePromptTemplate(prompt=prompt_temp_sistema)\n",
    "\n",
    "\n",
    "#Ahora para el humano\n",
    "prompt_temp_humano = PromptTemplate(template=\"{texto}\", input_variables=[\"texto\"])\n",
    "\n",
    "template_humano = HumanMessagePromptTemplate(prompt=prompt_temp_humano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([template_sistema, template_humano])\n",
    "\n",
    "# Este es el formato del prompt que acabamos de armar\n",
    "chat_promt_value = chat_prompt.format_prompt(adjetivo=\"economica\", texto=\"ipad\").to_messages()\n",
    "print(chat_promt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_resp = chatgpt(chat_promt_value)\n",
    "print(chat_resp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example selector\n",
    "Cuando estamos usando un modelo probablemente queremos darle un par de ejemplos para influir en el tipo de respuesta que nos brinda, con example selector podemos hacer esto de manera sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Primero hacemos una lista de  ejemplos de los que queremos que el modelo aprenda\n",
    "ejemplos = [\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal de la pizza?\", \"respuesta\": \"La masa y salsa de tomate\"},\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal de la hamburguesa?\", \"respuesta\": \"La carne y el pan\"},\n",
    "    {\"pregunta\": \"¿Cuál es el ingrediente principal del burrito?\", \"respuesta\": \"La tortilla y la carne\"}\n",
    "]\n",
    "\n",
    "# Ahora armamos un template para el modelo, como en los ejemplos incluimos respuesta en el template debe de ir un espacio para esta variable.\n",
    "# En template le decimos como queremos aque formatee el promt y de donde obtener las variables dentro de la variable ejemplos\n",
    "promt_temp_ejemplos = PromptTemplate(input_variables=[\"pregunta\", \"respuesta\"], \n",
    "                                     template = \"Pregunta: {pregunta}\\nRespuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_ejemplos = FewShotPromptTemplate(example_prompt=promt_temp_ejemplos, \n",
    "                                       examples=ejemplos, \n",
    "                                       prefix = \"Eres un asistenet virtual culinario que responde preguntas de manera muy breve\",\n",
    "                                       suffix = \"Pregunta: {pregunta}\\nRespuesta:\", \n",
    "                                        input_variables=[\"pregunta\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = promt_ejemplos.format(pregunta=\"¿Cuál es el ingrediente principal del coctel de camaron?\")\n",
    "print(prompt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_ingredientes = llm_openai(prompt_value)\n",
    "print(respuesta_ingredientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai(\"¿Cuál es el ingrediente principal del coctel de camaron?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai.get_num_tokens(prompt_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parser\n",
    "LangChain tambien nos da la oportunidad de parsear o formatear las respuestas que nos da el modelo de alguna manera que nos sea mas útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_basico_parser = \"\"\"Cuales son los ingredientes para preparar {platillo}\\n{como_parsear}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_parser = PromptTemplate(input_variables=[\"platillo\"], \n",
    "                                    template = template_basico_parser, \n",
    "                                    partial_variables={\"como_parsear\": format_instructions})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_value_parser = prompt_temp_parser.format(platillo=\"tacos al pastor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(promt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_parser = llm_openai(promt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(respuesta_parser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
